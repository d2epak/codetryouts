{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The file  Amendment Contract.pdf  has  289 words and  15  sentences in it.\n",
      "\n",
      "\n",
      "The 10 most common 3 word combinations appearing in this file are: \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "The file  Board Resolution - Approval of Option Grant Contract.pdf  has  350 words and  8  sentences in it.\n",
      "\n",
      "\n",
      "The 10 most common 3 word combinations appearing in this file are: \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "The file  docA.pdf  has  3263 words and  128  sentences in it.\n",
      "\n",
      "\n",
      "The 10 most common 3 word combinations appearing in this file are: \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "The file  ijfs-06-00007.pdf  has  8958 words and  515  sentences in it.\n",
      "\n",
      "\n",
      "The 10 most common 3 word combinations appearing in this file are: \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "The file  Software License Agreement Contract.pdf  has  5208 words and  267  sentences in it.\n",
      "\n",
      "\n",
      "The 10 most common 3 word combinations appearing in this file are: \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-559e4729172f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Code_Output/tfidfoutput.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f2' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "try:\n",
    "    import pdfminer\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'pdfminer'])\n",
    "finally:\n",
    "    import pdfminer\n",
    "\n",
    "try:\n",
    "    import sklearn as sk\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'sklearn'])\n",
    "finally:\n",
    "    import sklearn as sk\n",
    "    \n",
    "try:\n",
    "    import gensim as gn\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'gensim'])\n",
    "finally:\n",
    "    import gensim as gn\n",
    "    \n",
    "try:\n",
    "    import texthero as hero\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'texthero'])\n",
    "finally:\n",
    "    import texthero as hero\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",'-U', 'spacy'])\n",
    "finally:\n",
    "    import spacy\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'pandas'])\n",
    "finally:\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"python\", \"-m\", \"spacy\", 'download','en_core_web_sm'])\n",
    "finally:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", 'install','nltk'])\n",
    "finally:\n",
    "    import nltk\n",
    "\n",
    "\n",
    "\n",
    "#for reading the pdf\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "#for identifying and removing common english language words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#for identifying and printing 3 word and 4 word combinations with their frequency of occurence\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import QuadgramCollocationFinder\n",
    "\n",
    "\n",
    "#for counting the sentences and words\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "import collections\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#for couting most frequent words\n",
    "import re\n",
    "\n",
    "def convert(filename, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = open(filename, 'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text\n",
    "\n",
    "pdfFiles = []\n",
    "text_all = dict()\n",
    "actual_words_list = list()\n",
    "dir_name='./Source_Data/'\n",
    "#print('list directory', os.listdir(dir_name))\n",
    "\n",
    "for filename in os.listdir(dir_name):\n",
    "    #print('filename is ', filename)\n",
    "    if filename.endswith('.pdf') or filename.endswith('.PDF') or filename.endswith('.Pdf') or filename.endswith('.pDf') or filename.endswith('.pdF') or filename.endswith('.pDF') or filename.endswith('.pDf') or filename.endswith('.PDF'):\n",
    "         pdfFiles.append(filename)\n",
    "         text=convert(dir_name+filename)\n",
    "         k=filename\n",
    "         val=text\n",
    "         dict([(k, []) for k in text_all])\n",
    "\n",
    "         text_all[k] = val\n",
    "         text_all\n",
    "         \n",
    "         sentence_count = len(nltk.tokenize.sent_tokenize(text))\n",
    "         actual_words = nltk.tokenize.word_tokenize(text)\n",
    "         actual_words_list.append(actual_words)\n",
    "         \n",
    "         word_count = len(nltk.tokenize.word_tokenize(text))\n",
    "         print('\\nThe file ',filename,' has ',word_count, 'words and ', sentence_count,' sentences in it.\\n')\n",
    "         \n",
    "         #use findall for counting most common words, quadgrams, trigrams\n",
    "         all_text = re.findall(r'\\w+', text)\n",
    "         s=pd.Series(text)\n",
    "         all_text =map(lambda x: x.lower(), all_text)\n",
    "         filtered_words = list(filter(lambda word: word not in stopwords.words('english') and word.isalpha(), all_text))\n",
    "         #filtered_words = list(filter(lambda word: word not in stopwords.words('english'), all_text))\n",
    "\n",
    "         word_counts = Counter(filtered_words).most_common(20)\n",
    "         print('The 20 most commonly occuring words in this file are : \\n\\n', word_counts)\n",
    "         \n",
    "         print('\\nThe 10 most common 3 word combinations appearing in this file are: \\n')\n",
    "         trigram = TrigramCollocationFinder.from_words(filtered_words)\n",
    "         print(sorted(trigram.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:10])\n",
    "         \n",
    "         fourgrams=QuadgramCollocationFinder.from_words(filtered_words)\n",
    "         print('\\nThe 10 most common 4 word combinations appearing in this file are: \\n')\n",
    "         print(sorted(fourgrams.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:10])\n",
    "         \n",
    "         nlp = spacy.load(\"en_core_web_sm\")\n",
    "         doc = nlp(text)\n",
    "         \n",
    "         f = open(\"./Code_Output/spacy.txt\", \"a\", encoding='utf-8')\n",
    "         \n",
    "         ent_dict = {}\n",
    "   \n",
    "         for ent in doc.ents:\n",
    "            ent_dict.setdefault(ent.label_,[]).append(ent.text)\n",
    "    \n",
    "            for k,v in ent_dict.items():\n",
    "                ent_dict[k]=list(set(v))\n",
    "        \n",
    "         print('\\n Filename is:',filename, json.dumps(dict(ent_dict),indent=4), file=f)\n",
    "         #print('\\n Filename is:',filename, json.dumps(dict(ent_dict),indent=4))\n",
    "\n",
    "            \n",
    "         f1 = open(\"./Code_Output/texthero.txt\", \"a\", encoding='utf-8')\n",
    "         \n",
    "         print('\\nFilename is ',filename, file=f1)\n",
    "         print('\\n TFIDF Values are: ', (hero.tfidf(pd.Series(filtered_words), return_feature_names=True)), file=f1)\n",
    "         print('\\n Term Frequency Values are: ',hero.term_frequency(pd.Series(filtered_words), return_feature_names=True), file=f1)\n",
    "         print('\\n Top Words not-normalized are: ',hero.top_words(pd.Series(filtered_words), normalize=False), file=f1)\n",
    "         print('\\n TFIDF Words normalized are: ',hero.top_words(pd.Series(filtered_words), normalize=True), file=f1)\n",
    "         \n",
    "         \n",
    "         print('----------------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "actual_words_list=sum(actual_words_list,[])\n",
    "myvocabulary = list(set(actual_words_list)) #['agreement', 'risk']\n",
    "corpus = text_all #{1: \"The game of life is a game of everlasting learning\", 2: \"The unexamined life is not worth living\", 3: \"Never stop learning\"}\n",
    "tfidf = TfidfVectorizer(vocabulary = myvocabulary, ngram_range = (1,2),stop_words='english')\n",
    "tfs = tfidf.fit_transform(corpus.values())\n",
    "df = pd.DataFrame(tfs.todense(), index=corpus.keys(), columns=myvocabulary) #, index=corpus_index, columns=feature_names)\n",
    "df=df.transpose()\n",
    "df=df.loc[~(df==0).all(axis=1)]\n",
    "df.to_csv('./Code_Output/tfidfoutput.csv')\n",
    "#print(df, file=f2)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amendment Contract.pdf</th>\n",
       "      <th>Board Resolution - Approval of Option Grant Contract.pdf</th>\n",
       "      <th>docA.pdf</th>\n",
       "      <th>ijfs-06-00007.pdf</th>\n",
       "      <th>Software License Agreement Contract.pdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>written</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>award</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025314</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclusive</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>defects</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lack</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carried</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>0.005481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065816</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1878 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Amendment Contract.pdf  \\\n",
       "written                       0.0   \n",
       "delle                         0.0   \n",
       "concept                       0.0   \n",
       "award                         0.0   \n",
       "2012                          0.0   \n",
       "...                           ...   \n",
       "exclusive                     0.0   \n",
       "defects                       0.0   \n",
       "lack                          0.0   \n",
       "carried                       0.0   \n",
       "macro                         0.0   \n",
       "\n",
       "           Board Resolution - Approval of Option Grant Contract.pdf  docA.pdf  \\\n",
       "written                                             0.000000         0.054627   \n",
       "delle                                               0.000000         0.000000   \n",
       "concept                                             0.000000         0.000000   \n",
       "award                                               0.000000         0.046823   \n",
       "2012                                                0.000000         0.000000   \n",
       "...                                                      ...              ...   \n",
       "exclusive                                           0.000000         0.000000   \n",
       "defects                                             0.000000         0.000000   \n",
       "lack                                                0.045091         0.000000   \n",
       "carried                                             0.000000         0.000000   \n",
       "macro                                               0.000000         0.000000   \n",
       "\n",
       "           ijfs-06-00007.pdf  Software License Agreement Contract.pdf  \n",
       "written             0.000000                                 0.021923  \n",
       "delle               0.010126                                 0.000000  \n",
       "concept             0.035439                                 0.000000  \n",
       "award               0.000000                                 0.016442  \n",
       "2012                0.025314                                 0.000000  \n",
       "...                      ...                                      ...  \n",
       "exclusive           0.000000                                 0.054345  \n",
       "defects             0.000000                                 0.013586  \n",
       "lack                0.004085                                 0.000000  \n",
       "carried             0.020423                                 0.005481  \n",
       "macro               0.065816                                 0.000000  \n",
       "\n",
       "[1878 rows x 5 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(vocabulary = myvocabulary, ngram_range = (1,2),stop_words='english')\n",
    "tfs = tfidf.fit_transform(corpus.values())\n",
    "df = pd.DataFrame(tfs.todense(), index=corpus.keys(), columns=myvocabulary) #, index=corpus_index, columns=feature_names)\n",
    "df=df.transpose()\n",
    "df=df.loc[~(df==0).all(axis=1)]\n",
    "df\n",
    "#import seaborn as sns\n",
    "#sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
